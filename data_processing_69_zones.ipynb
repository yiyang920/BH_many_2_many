{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import os\n",
    "\n",
    "os.environ[\"PROJ_LIB\"] = r\"C:\\\\Users\\\\SQwan\\\\miniconda3\\\\Library\\\\share\"\n",
    "import pickle\n",
    "import googlemaps\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import csv\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "from shapely.geometry import Point, shape, Polygon\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "import shapely.speedups\n",
    "\n",
    "shapely.speedups.enable()\n",
    "\n",
    "if not os.path.exists(r\"Data\"):\n",
    "    os.makedirs(r\"Data\")\n",
    "\n",
    "if not os.path.exists(r\"Data\\temp\"):\n",
    "    os.makedirs(r\"Data\\temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crs': {'init': 'epsg:4269'},\n",
      " 'crs_wkt': 'GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS '\n",
      "            '1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4269\"]]',\n",
      " 'driver': 'ESRI Shapefile',\n",
      " 'schema': {'geometry': 'Polygon',\n",
      "            'properties': OrderedDict([('STATEFP', 'str:2'),\n",
      "                                       ('COUNTYFP', 'str:3'),\n",
      "                                       ('TRACTCE', 'str:6'),\n",
      "                                       ('BLKGRPCE', 'str:1'),\n",
      "                                       ('GEOID', 'str:12'),\n",
      "                                       ('NAMELSAD', 'str:13'),\n",
      "                                       ('MTFCC', 'str:5'),\n",
      "                                       ('FUNCSTAT', 'str:1'),\n",
      "                                       ('ALAND', 'int:14'),\n",
      "                                       ('AWATER', 'int:14'),\n",
      "                                       ('INTPTLAT', 'str:11'),\n",
      "                                       ('INTPTLON', 'str:12'),\n",
      "                                       ('in_region', 'float:2.1'),\n",
      "                                       ('zone_id', 'int:4')])}}\n"
     ]
    }
   ],
   "source": [
    "fileloc = r\"Data\\\\shapefile\\\\destinations_in_region\\\\destinations_in_region.shp\"\n",
    "with fiona.open(fileloc) as fc:\n",
    "    pprint(fc.meta)\n",
    "#     pprint(fc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsg:4269\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATEFP</th>\n",
       "      <th>COUNTYFP</th>\n",
       "      <th>TRACTCE</th>\n",
       "      <th>BLKGRPCE</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>NAMELSAD</th>\n",
       "      <th>MTFCC</th>\n",
       "      <th>FUNCSTAT</th>\n",
       "      <th>ALAND</th>\n",
       "      <th>AWATER</th>\n",
       "      <th>INTPTLAT</th>\n",
       "      <th>INTPTLON</th>\n",
       "      <th>in_region</th>\n",
       "      <th>zone_id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>26</td>\n",
       "      <td>021</td>\n",
       "      <td>000300</td>\n",
       "      <td>2</td>\n",
       "      <td>260210003002</td>\n",
       "      <td>Block Group 2</td>\n",
       "      <td>G5030</td>\n",
       "      <td>S</td>\n",
       "      <td>2959600</td>\n",
       "      <td>24858</td>\n",
       "      <td>+42.1278772</td>\n",
       "      <td>-086.4282937</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>POLYGON ((-86.45313 42.12005, -86.45295 42.120...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>26</td>\n",
       "      <td>021</td>\n",
       "      <td>002000</td>\n",
       "      <td>3</td>\n",
       "      <td>260210020003</td>\n",
       "      <td>Block Group 3</td>\n",
       "      <td>G5030</td>\n",
       "      <td>S</td>\n",
       "      <td>3703886</td>\n",
       "      <td>101660</td>\n",
       "      <td>+42.0711821</td>\n",
       "      <td>-086.4474414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((-86.45593 42.08707, -86.45125 42.087...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>26</td>\n",
       "      <td>021</td>\n",
       "      <td>001700</td>\n",
       "      <td>1</td>\n",
       "      <td>260210017001</td>\n",
       "      <td>Block Group 1</td>\n",
       "      <td>G5030</td>\n",
       "      <td>S</td>\n",
       "      <td>3885199</td>\n",
       "      <td>3066</td>\n",
       "      <td>+42.0361694</td>\n",
       "      <td>-086.4800761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((-86.48508 42.02959, -86.48507 42.032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>26</td>\n",
       "      <td>021</td>\n",
       "      <td>002200</td>\n",
       "      <td>3</td>\n",
       "      <td>260210022003</td>\n",
       "      <td>Block Group 3</td>\n",
       "      <td>G5030</td>\n",
       "      <td>S</td>\n",
       "      <td>2580772</td>\n",
       "      <td>0</td>\n",
       "      <td>+42.1092878</td>\n",
       "      <td>-086.4075581</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((-86.41747 42.11596, -86.41631 42.115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>26</td>\n",
       "      <td>021</td>\n",
       "      <td>002000</td>\n",
       "      <td>2</td>\n",
       "      <td>260210020002</td>\n",
       "      <td>Block Group 2</td>\n",
       "      <td>G5030</td>\n",
       "      <td>S</td>\n",
       "      <td>640226</td>\n",
       "      <td>0</td>\n",
       "      <td>+42.0907256</td>\n",
       "      <td>-086.4511974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>POLYGON ((-86.45598 42.09441, -86.45425 42.094...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    STATEFP COUNTYFP TRACTCE BLKGRPCE         GEOID       NAMELSAD  MTFCC  \\\n",
       "520      26      021  000300        2  260210003002  Block Group 2  G5030   \n",
       "521      26      021  002000        3  260210020003  Block Group 3  G5030   \n",
       "522      26      021  001700        1  260210017001  Block Group 1  G5030   \n",
       "524      26      021  002200        3  260210022003  Block Group 3  G5030   \n",
       "527      26      021  002000        2  260210020002  Block Group 2  G5030   \n",
       "\n",
       "    FUNCSTAT    ALAND  AWATER     INTPTLAT      INTPTLON  in_region  zone_id  \\\n",
       "520        S  2959600   24858  +42.1278772  -086.4282937        1.0        0   \n",
       "521        S  3703886  101660  +42.0711821  -086.4474414        1.0        1   \n",
       "522        S  3885199    3066  +42.0361694  -086.4800761        1.0        2   \n",
       "524        S  2580772       0  +42.1092878  -086.4075581        1.0        3   \n",
       "527        S   640226       0  +42.0907256  -086.4511974        1.0        4   \n",
       "\n",
       "                                              geometry  \n",
       "520  POLYGON ((-86.45313 42.12005, -86.45295 42.120...  \n",
       "521  POLYGON ((-86.45593 42.08707, -86.45125 42.087...  \n",
       "522  POLYGON ((-86.48508 42.02959, -86.48507 42.032...  \n",
       "524  POLYGON ((-86.41747 42.11596, -86.41631 42.115...  \n",
       "527  POLYGON ((-86.45598 42.09441, -86.45425 42.094...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "poly_BH = gpd.read_file(fileloc)\n",
    "\n",
    "poly_BH = poly_BH.loc[poly_BH.in_region==1]\n",
    "print(poly_BH.crs)\n",
    "_ = poly_BH.plot()\n",
    "poly_BH[\"zone_id\"] = np.arange(len(poly_BH))\n",
    "poly_BH.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-86.623997 -86.320005 41.899257999999996 42.24322\n"
     ]
    }
   ],
   "source": [
    "poly_BH_summary = poly_BH.geometry.bounds\n",
    "lolon, uplon, lolat, uplat = poly_BH_summary.minx.min(\n",
    "), poly_BH_summary.maxx.max(), poly_BH_summary.miny.min(), poly_BH_summary.maxy.max()\n",
    "print(lolon, uplon, lolat, uplat)\n",
    "\n",
    "with open(r\"Data\\temp\\boundary.p\", \"wb\") as fp:\n",
    "    pickle.dump([lolon, uplon, lolat, uplat], fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find centroids of BH TAZs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lolon, uplon, lolat, uplat = -86.63, -86.31, 41.89, 42.15\n",
    "bm = Basemap(\n",
    "    llcrnrlon=lolon,\n",
    "    llcrnrlat=lolat,\n",
    "    urcrnrlon=uplon,\n",
    "    urcrnrlat=uplat,\n",
    "    resolution=\"i\",\n",
    "    projection=\"tmerc\",\n",
    "    lat_0=42.07,\n",
    "    lon_0=-86.45,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-3e7a439406dc>:4: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroids_BH.geometry = centroids_BH['geometry'].centroid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATEFP</th>\n",
       "      <th>COUNTYFP</th>\n",
       "      <th>TRACTCE</th>\n",
       "      <th>BLKGRPCE</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>NAMELSAD</th>\n",
       "      <th>MTFCC</th>\n",
       "      <th>FUNCSTAT</th>\n",
       "      <th>ALAND</th>\n",
       "      <th>AWATER</th>\n",
       "      <th>INTPTLAT</th>\n",
       "      <th>INTPTLON</th>\n",
       "      <th>in_region</th>\n",
       "      <th>zone_id</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>26</td>\n",
       "      <td>021</td>\n",
       "      <td>000300</td>\n",
       "      <td>2</td>\n",
       "      <td>260210003002</td>\n",
       "      <td>Block Group 2</td>\n",
       "      <td>G5030</td>\n",
       "      <td>S</td>\n",
       "      <td>2959600</td>\n",
       "      <td>24858</td>\n",
       "      <td>+42.1278772</td>\n",
       "      <td>-086.4282937</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>POINT (-86.43412 42.12652)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>26</td>\n",
       "      <td>021</td>\n",
       "      <td>002000</td>\n",
       "      <td>3</td>\n",
       "      <td>260210020003</td>\n",
       "      <td>Block Group 3</td>\n",
       "      <td>G5030</td>\n",
       "      <td>S</td>\n",
       "      <td>3703886</td>\n",
       "      <td>101660</td>\n",
       "      <td>+42.0711821</td>\n",
       "      <td>-086.4474414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>POINT (-86.44610 42.07618)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>26</td>\n",
       "      <td>021</td>\n",
       "      <td>001700</td>\n",
       "      <td>1</td>\n",
       "      <td>260210017001</td>\n",
       "      <td>Block Group 1</td>\n",
       "      <td>G5030</td>\n",
       "      <td>S</td>\n",
       "      <td>3885199</td>\n",
       "      <td>3066</td>\n",
       "      <td>+42.0361694</td>\n",
       "      <td>-086.4800761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>POINT (-86.47533 42.03988)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>26</td>\n",
       "      <td>021</td>\n",
       "      <td>002200</td>\n",
       "      <td>3</td>\n",
       "      <td>260210022003</td>\n",
       "      <td>Block Group 3</td>\n",
       "      <td>G5030</td>\n",
       "      <td>S</td>\n",
       "      <td>2580772</td>\n",
       "      <td>0</td>\n",
       "      <td>+42.1092878</td>\n",
       "      <td>-086.4075581</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>POINT (-86.40756 42.10929)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>26</td>\n",
       "      <td>021</td>\n",
       "      <td>002000</td>\n",
       "      <td>2</td>\n",
       "      <td>260210020002</td>\n",
       "      <td>Block Group 2</td>\n",
       "      <td>G5030</td>\n",
       "      <td>S</td>\n",
       "      <td>640226</td>\n",
       "      <td>0</td>\n",
       "      <td>+42.0907256</td>\n",
       "      <td>-086.4511974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>POINT (-86.45120 42.09073)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    STATEFP COUNTYFP TRACTCE BLKGRPCE         GEOID       NAMELSAD  MTFCC  \\\n",
       "520      26      021  000300        2  260210003002  Block Group 2  G5030   \n",
       "521      26      021  002000        3  260210020003  Block Group 3  G5030   \n",
       "522      26      021  001700        1  260210017001  Block Group 1  G5030   \n",
       "524      26      021  002200        3  260210022003  Block Group 3  G5030   \n",
       "527      26      021  002000        2  260210020002  Block Group 2  G5030   \n",
       "\n",
       "    FUNCSTAT    ALAND  AWATER     INTPTLAT      INTPTLON  in_region  zone_id  \\\n",
       "520        S  2959600   24858  +42.1278772  -086.4282937        1.0        0   \n",
       "521        S  3703886  101660  +42.0711821  -086.4474414        1.0        1   \n",
       "522        S  3885199    3066  +42.0361694  -086.4800761        1.0        2   \n",
       "524        S  2580772       0  +42.1092878  -086.4075581        1.0        3   \n",
       "527        S   640226       0  +42.0907256  -086.4511974        1.0        4   \n",
       "\n",
       "                       geometry  \n",
       "520  POINT (-86.43412 42.12652)  \n",
       "521  POINT (-86.44610 42.07618)  \n",
       "522  POINT (-86.47533 42.03988)  \n",
       "524  POINT (-86.40756 42.10929)  \n",
       "527  POINT (-86.45120 42.09073)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy poly to new GeoDataFrame\n",
    "centroids_BH = poly_BH.copy()\n",
    "# change the geometry to centroid of each polygon\n",
    "centroids_BH.geometry = centroids_BH['geometry'].centroid\n",
    "# same crs\n",
    "centroids_BH.crs = poly_BH.crs\n",
    "centroids_BH.rename(columns={\"ID\": \"GEOID\"}, inplace=True)\n",
    "centroids_BH.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-b635acbf419e>:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid_df.geometry = centroid_df.centroid\n"
     ]
    }
   ],
   "source": [
    "# extract geometry, geoid, and zone id\n",
    "poly_df = poly_BH[[\"geometry\", \"GEOID\", \"zone_id\"]]\n",
    "# copy poly series to new GeoDataFrame\n",
    "centroid_df = poly_df.copy(deep=True)\n",
    "# change the geometry to centroid of each polygon\n",
    "centroid_df.geometry = centroid_df.centroid\n",
    "# same crs\n",
    "centroid_df.crs = poly_df.crs\n",
    "\n",
    "fig = plt.figure(figsize=(50, 50))\n",
    "ax = fig.gca()\n",
    "\n",
    "_ = poly_df.boundary.plot(ax=ax, color=\"blue\")\n",
    "_ = centroid_df.plot(ax=ax, marker=\".\", color=\"black\")\n",
    "\n",
    "for i, (xx, yy) in enumerate(\n",
    "    zip(centroid_df.geometry.x, centroid_df.geometry.y), start=0\n",
    "):\n",
    "    plt.annotate(\n",
    "        str(centroid_df.zone_id.iloc[i]),\n",
    "        (xx, yy),\n",
    "        xytext=(5, 5),\n",
    "        textcoords=\"offset points\",\n",
    "    )\n",
    "\n",
    "# Uncomment the next line to save the figure\n",
    "# fig.savefig(r\"figure\\zones_number.png\",  bbox_inches=\"tight\", dpi=100)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>zone_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>POINT (-86.43412 42.12652)</td>\n",
       "      <td>260210003002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>POINT (-86.44610 42.07618)</td>\n",
       "      <td>260210020003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>POINT (-86.47533 42.03988)</td>\n",
       "      <td>260210017001</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>POINT (-86.40756 42.10929)</td>\n",
       "      <td>260210022003</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>POINT (-86.45120 42.09073)</td>\n",
       "      <td>260210020002</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7806</th>\n",
       "      <td>POINT (-86.48083 42.08443)</td>\n",
       "      <td>260210010002</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7837</th>\n",
       "      <td>POINT (-86.44103 42.11146)</td>\n",
       "      <td>260210003001</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7845</th>\n",
       "      <td>POINT (-86.46896 42.06576)</td>\n",
       "      <td>260210011001</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7883</th>\n",
       "      <td>POINT (-86.40573 42.17035)</td>\n",
       "      <td>260210101001</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7884</th>\n",
       "      <td>POINT (-86.35630 42.19136)</td>\n",
       "      <td>260210101004</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        geometry         GEOID  zone_id\n",
       "520   POINT (-86.43412 42.12652)  260210003002        0\n",
       "521   POINT (-86.44610 42.07618)  260210020003        1\n",
       "522   POINT (-86.47533 42.03988)  260210017001        2\n",
       "524   POINT (-86.40756 42.10929)  260210022003        3\n",
       "527   POINT (-86.45120 42.09073)  260210020002        4\n",
       "...                          ...           ...      ...\n",
       "7806  POINT (-86.48083 42.08443)  260210010002       64\n",
       "7837  POINT (-86.44103 42.11146)  260210003001       65\n",
       "7845  POINT (-86.46896 42.06576)  260210011001       66\n",
       "7883  POINT (-86.40573 42.17035)  260210101001       67\n",
       "7884  POINT (-86.35630 42.19136)  260210101004       68\n",
       "\n",
       "[69 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zone ID Shapefile and ID Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(poly_df, crs=poly_df.crs)\n",
    "gdf[\"centroid\"] = poly_df.centroid.apply(lambda x: str((x.x, x.y)))\n",
    "\n",
    "try:\n",
    "    gdf.to_file(r\"Data\\shapefile\\zone_id.shp\")\n",
    "except:\n",
    "    os.makedirs(r\"Data\\shapefile\")\n",
    "    gdf.to_file(r\"Data\\shapefile\\zone_id.shp\")\n",
    "\n",
    "id_converter = {i: int(j) for (i, j) in zip(gdf[\"zone_id\"], gdf[\"GEOID\"])}\n",
    "id_converter_reverse = {v: k for k, v in id_converter.items()}\n",
    "with open(r\"Data\\id_converter.p\", \"wb\") as fp:\n",
    "    pickle.dump(id_converter, fp)\n",
    "with open(r\"Data\\id_converter_reverse.p\", \"wb\") as fp:\n",
    "    pickle.dump(id_converter_reverse, fp)\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"lon\", \"lat\"])\n",
    "df.index.name = \"zone_id\"\n",
    "df[\"lon\"], df[\"lat\"] = (centroid_df.geometry.x,\n",
    "                        centroid_df.geometry.y)\n",
    "df.index = centroid_df.zone_id\n",
    "df[\"GEOID\"] = [id_converter[i] for i in df.index]\n",
    "df.to_csv(r\"Data\\zone_id.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate station data and find neighbor stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get utm coordinates for the centroids\n",
    "c_lon, c_lat = bm(centroid_df.geometry.x,\n",
    "                  centroid_df.geometry.y, inverse=False)\n",
    "# reverse lat lon order used for googlemaps API\n",
    "lat_lon = np.c_[c_lat, c_lon]\n",
    "station = dict()\n",
    "for i, center in zip(centroid_df.zone_id, centroid_df.geometry):\n",
    "    station[i] = dict()\n",
    "    station[i][\"lat_lon\"] = [\n",
    "        centroid_df.geometry.iloc[i].y,\n",
    "        centroid_df.geometry.iloc[i].x]\n",
    "    station[i][\"utm\"] = lat_lon[i]\n",
    "    station[i][\"geoid\"] = centroid_df.GEOID.iloc[i]\n",
    "    neighbors = poly_df.zone_id[~poly_df.disjoint(\n",
    "        poly_df.geometry.iloc[i])]\n",
    "    # exclude self node in neighbours\n",
    "    station[i][\"neighbours\"] = [n for n in neighbors if n != i]\n",
    "\n",
    "pickle.dump(station, open(r\"Data\\temp\\Station.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(itertools.islice(station.items(), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use your google map API key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your google map API key\n",
    "gmap_key = \"AIzaSyCAMHXBSQrYIppBxu_GAKCMhR4rYSPuB8k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = pickle.load(open(r\"Data\\temp\\Station.p\", \"rb\"))\n",
    "num_station = len(station)\n",
    "gmaps = googlemaps.Client(key=gmap_key)\n",
    "# datetime needs to be no later than current time, format: Y,M,D,h,m,s\n",
    "currenttime = datetime(2021, 4, 1, 12, 0, 0)\n",
    "\n",
    "# construct graphs of travel time and travel distance\n",
    "G_d = nx.Graph()\n",
    "G_t = nx.Graph()\n",
    "G_d.add_nodes_from(np.arange(num_station))\n",
    "G_t.add_nodes_from(np.arange(num_station))\n",
    "\n",
    "for i in range(num_station):\n",
    "    for j in station[i][\"neighbours\"]:\n",
    "        if i < j:\n",
    "            direction_result = gmaps.directions(\n",
    "                tuple(station[i][\"lat_lon\"]),\n",
    "                tuple(station[j][\"lat_lon\"]),\n",
    "                mode=\"driving\",\n",
    "                avoid=\"ferries\",\n",
    "                departure_time=currenttime,\n",
    "            )\n",
    "\n",
    "            w_t = np.round(\n",
    "                direction_result[0][\"legs\"][0][\"duration\"][u\"value\"] / 60.0, decimals=2\n",
    "            )\n",
    "            w_d = np.round(\n",
    "                direction_result[0][\"legs\"][0][\"distance\"][u\"value\"] *\n",
    "                0.000621371,\n",
    "                decimals=2,\n",
    "            )\n",
    "\n",
    "            G_t.add_edge(i, j, weight=w_t)\n",
    "            G_d.add_edge(i, j, weight=w_d)\n",
    "\n",
    "pickle.dump(G_t, open(r\"Data\\temp\\G_t.p\", \"wb\"))\n",
    "pickle.dump(G_d, open(r\"Data\\temp\\G_d.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with edge cost as shortest travel time (min)\n",
    "G_t = pickle.load(open(r\"Data\\temp\\G_t.p\", \"rb\"))\n",
    "# graph with edge cost as shortest travel distance (km)\n",
    "G_d = pickle.load(open(r\"Data\\temp\\G_d.p\", \"rb\"))\n",
    "\n",
    "GraphWeights_t = [(u, v, w[\"weight\"]) for (u, v, w) in G_t.edges(data=True)]\n",
    "name = [\"origin\", \"destination\", \"travel time(min)\"]\n",
    "df_t = pd.DataFrame(data=GraphWeights_t, columns=name)\n",
    "\n",
    "GraphWeights_d = [(u, v, w[\"weight\"]) for (u, v, w) in G_d.edges(data=True)]\n",
    "name_d = [\"origin\", \"destination\", \"travel distance(km)\"]\n",
    "df_d = pd.DataFrame(data=GraphWeights_d, columns=name_d)\n",
    "\n",
    "df = df_t.merge(df_d, how='outer', on=[\"origin\", \"destination\"])\n",
    "\n",
    "try:\n",
    "    df.to_csv(r\"Data\\census\\station_distance.csv\", sep=\",\")\n",
    "except FileNotFoundError:\n",
    "    if not os.path.exists('Data\\census'):\n",
    "        os.makedirs('Data\\census')\n",
    "    df.to_csv(r\"Data\\census\\station_distance.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting distance table to a matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distance = pd.read_csv(r\"Data\\census\\station_distance.csv\", index_col=[0])\n",
    "df_distance_mtx = pd.pivot_table(\n",
    "    df_distance, index='origin', columns='destination')\n",
    "\n",
    "df_distance_mtx.to_csv(r\"Data\\census\\station_distance_mtx.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with edge cost as shortest travel time (min)\n",
    "G_t = pickle.load(open(r\"Data\\temp\\G_t.p\", \"rb\"))\n",
    "# graph with edge cost as shortest travel distance (km)\n",
    "G_d = pickle.load(open(r\"Data\\temp\\G_d.p\", \"rb\"))\n",
    "\n",
    "id_converter = pickle.load(open(r\"Data\\id_converter.p\", \"rb\"))\n",
    "\n",
    "S = len(poly_df)\n",
    "\n",
    "df_time = pd.DataFrame()\n",
    "df_dist = pd.DataFrame()\n",
    "\n",
    "for i in poly_df[\"zone_id\"]:\n",
    "    for j in poly_df[\"zone_id\"]:\n",
    "        if i == j:\n",
    "            df_time.loc[id_converter[i], id_converter[j]] = 0\n",
    "            df_dist.loc[id_converter[i], id_converter[j]] = 0\n",
    "        else:\n",
    "            df_time.loc[\n",
    "                id_converter[i], id_converter[j]\n",
    "            ] = nx.shortest_path_length(\n",
    "                G_t, source=i, target=j, weight='weight')\n",
    "            df_dist.loc[\n",
    "                id_converter[i], id_converter[j]\n",
    "            ] = nx.shortest_path_length(\n",
    "                G_d, source=i, target=j, weight='weight')\n",
    "\n",
    "df_time.to_csv(r\"Data\\gm_time_min.csv\")\n",
    "df_dist.to_csv(r\"Data\\gm_dist_km.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zone Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [31, 30, 4, 7]\n",
    "l2 = [32, 33, 34, 35]\n",
    "l3 = [0, 42, 5, 40]\n",
    "l4 = [50, 36, 6, 64]\n",
    "l5 = [13, 14]\n",
    "l6 = [54, 55, 56, 61]\n",
    "l7 = [28, 46]\n",
    "l8 = [65, 49, 8]\n",
    "l9 = [57, 59, 60]\n",
    "l10 = [27, 47]\n",
    "l11 = [11, 51]\n",
    "l12 = [62, 63]\n",
    "l13 = [20, 22]\n",
    "l14 = [15, 2, 16, 17]\n",
    "l15 = [21, 23, 24]\n",
    "\n",
    "agg_list = [\n",
    "    l1,\n",
    "    l2,\n",
    "    l3,\n",
    "    l4,\n",
    "    l5,\n",
    "    l6,\n",
    "    l7,\n",
    "    l8,\n",
    "    l9,\n",
    "    l10,\n",
    "    l11,\n",
    "    l12,\n",
    "    l13,\n",
    "    l14,\n",
    "    l15,\n",
    "]\n",
    "agg_list = [lst for lst in agg_list if lst != []]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checker function, make sure there is no common value in any pair of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in itertools.product(agg_list, repeat=2):\n",
    "    if pair[0] != pair[1] and list(set.intersection(*map(set, pair))) != []:\n",
    "        print(\"not disjoint list, intersection\",\n",
    "              list(set.intersection(*map(set, pair))), pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_id = gpd.read_file(r\"Data\\shapefile\\zone_id.shp\")\n",
    "zone_id['group'] = zone_id['zone_id']\n",
    "\n",
    "agg_2_disagg = {min(lst): lst for lst in agg_list}\n",
    "\n",
    "for key, lst in agg_2_disagg.items():\n",
    "    zone_id['group'].replace(to_replace=lst, value=key, inplace=True)\n",
    "\n",
    "zone_agg = zone_id.dissolve(by='group')\n",
    "len(zone_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup table from aggregated zone id to a list of original zone ids and its reverse table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_2_disagg_id = {i: [zone_agg.index[i],] for i in range(len(zone_agg))}\n",
    "for key, value in agg_2_disagg_id.items():\n",
    "    if value[0] in agg_2_disagg:\n",
    "        agg_2_disagg_id[key] = agg_2_disagg[value[0]]\n",
    "\n",
    "zone_agg[\"zone_id\"] = np.arange(len(zone_agg))\n",
    "zone_agg.to_csv(r\"Data\\zone_id_agg.csv\")\n",
    "pickle.dump(agg_2_disagg_id, open(r\"Data\\agg_2_disagg_id.p\", \"wb\"))\n",
    "\n",
    "disagg_2_agg_id = dict()\n",
    "for zone_agg, zone_list in agg_2_disagg_id.items():\n",
    "    for zone in zone_list:\n",
    "        disagg_2_agg_id[zone] = zone_agg\n",
    "pickle.dump(disagg_2_agg_id, open(r\"Data\\disagg_2_agg_id.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    zone_agg.to_file(r\"Data\\shapefile\\zone_id_agg.shp\")\n",
    "except:\n",
    "    os.makedirs(r\"Data\\shapefile\")\n",
    "    zone_agg.to_file(r\"Data\\shapefile\\zone_id_agg.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID converter for aggregated zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_converter_agg = {i: int(j) for (i, j) in zip(zone_agg[\"zone_id\"], zone_agg[\"GEOID\"])}\n",
    "id_converter_reverse_agg = {v: k for k, v in id_converter_agg.items()}\n",
    "with open(r\"Data\\id_converter_agg.p\", \"wb\") as fp:\n",
    "    pickle.dump(id_converter_agg, fp)\n",
    "with open(r\"Data\\id_converter_reverse_agg.p\", \"wb\") as fp:\n",
    "    pickle.dump(id_converter_reverse_agg, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate station data and find neighbor stations for aggregated zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy poly series to new GeoDataFrame\n",
    "centroid_df = zone_agg.copy(deep=True)\n",
    "# change the geometry to centroid of each polygon\n",
    "centroid_df.geometry = centroid_df.centroid\n",
    "# same crs\n",
    "centroid_df.crs = zone_agg.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_2_disagg_id = pickle.load(open(r\"Data\\agg_2_disagg_id.p\", \"rb\"))\n",
    "# get utm coordinates for the centroids\n",
    "c_lon, c_lat = bm(centroid_df.geometry.x, centroid_df.geometry.y, inverse=False)\n",
    "# reverse lat lon order used for googlemaps API\n",
    "lat_lon = np.c_[c_lat, c_lon]\n",
    "station = dict()\n",
    "for i, center in zip(centroid_df.zone_id, centroid_df.geometry):\n",
    "    station[i] = dict()\n",
    "    station[i][\"lat_lon\"] = [\n",
    "        centroid_df.geometry.iloc[i].y,\n",
    "        centroid_df.geometry.iloc[i].x,\n",
    "    ]\n",
    "    station[i][\"utm\"] = lat_lon[i]\n",
    "    station[i][\"geoid\"] = centroid_df.GEOID.iloc[i]\n",
    "    neighbors = zone_agg.zone_id[~zone_agg.disjoint(zone_agg.geometry.iloc[i])]\n",
    "    # exclude self node in neighbours\n",
    "    station[i][\"neighbours\"] = [n for n in neighbors if n != i]\n",
    "    station[i][\"disagg_id\"] = agg_2_disagg_id[i]\n",
    "\n",
    "pickle.dump(station, open(r\"Data\\temp\\Station_agg.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(itertools.islice(station.items(), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use your google map API key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your google map API key\n",
    "gmap_key = \"AIzaSyCAMHXBSQrYIppBxu_GAKCMhR4rYSPuB8k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = pickle.load(open(r\"Data\\temp\\Station_agg.p\", \"rb\"))\n",
    "num_station = len(station)\n",
    "gmaps = googlemaps.Client(key=gmap_key)\n",
    "# datetime needs to be no later than current time, format: Y,M,D,h,m,s\n",
    "currenttime = datetime(2021, 4, 1, 12, 0, 0)\n",
    "\n",
    "# construct graphs of travel time and travel distance\n",
    "G_d = nx.Graph()\n",
    "G_t = nx.Graph()\n",
    "G_d.add_nodes_from(np.arange(num_station))\n",
    "G_t.add_nodes_from(np.arange(num_station))\n",
    "\n",
    "for i in range(num_station):\n",
    "    for j in station[i][\"neighbours\"]:\n",
    "        if i < j:\n",
    "            direction_result = gmaps.directions(\n",
    "                tuple(station[i][\"lat_lon\"]),\n",
    "                tuple(station[j][\"lat_lon\"]),\n",
    "                mode=\"driving\",\n",
    "                avoid=\"ferries\",\n",
    "                departure_time=currenttime,\n",
    "            )\n",
    "\n",
    "            w_t = np.round(\n",
    "                direction_result[0][\"legs\"][0][\"duration\"][u\"value\"] / 60.0, decimals=2\n",
    "            )\n",
    "            w_d = np.round(\n",
    "                direction_result[0][\"legs\"][0][\"distance\"][u\"value\"] *\n",
    "                0.000621371,\n",
    "                decimals=2,\n",
    "            )\n",
    "\n",
    "            G_t.add_edge(i, j, weight=w_t)\n",
    "            G_d.add_edge(i, j, weight=w_d)\n",
    "\n",
    "pickle.dump(G_t, open(r\"Data\\temp\\G_t_agg.p\", \"wb\"))\n",
    "pickle.dump(G_d, open(r\"Data\\temp\\G_d_agg.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with edge cost as shortest travel time (min)\n",
    "G_t_agg = pickle.load(open(r\"Data\\temp\\G_t_agg.p\", \"rb\"))\n",
    "# graph with edge cost as shortest travel distance (km)\n",
    "G_d_agg = pickle.load(open(r\"Data\\temp\\G_d_agg.p\", \"rb\"))\n",
    "\n",
    "GraphWeights_t = [(u, v, w[\"weight\"]) for (u, v, w) in G_t_agg.edges(data=True)]\n",
    "name = [\"origin\", \"destination\", \"travel time(min)\"]\n",
    "df_t = pd.DataFrame(data=GraphWeights_t, columns=name)\n",
    "\n",
    "GraphWeights_d = [(u, v, w[\"weight\"]) for (u, v, w) in G_d_agg.edges(data=True)]\n",
    "name_d = [\"origin\", \"destination\", \"travel distance(km)\"]\n",
    "df_d = pd.DataFrame(data=GraphWeights_d, columns=name_d)\n",
    "\n",
    "df = df_t.merge(df_d, how='outer', on=[\"origin\", \"destination\"])\n",
    "\n",
    "try:\n",
    "    df.to_csv(r\"Data\\census\\station_distance_agg.csv\", sep=\",\")\n",
    "except FileNotFoundError:\n",
    "    if not os.path.exists('Data\\census'):\n",
    "        os.makedirs('Data\\census')\n",
    "    df.to_csv(r\"Data\\census\\station_distance_agg.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting distance table to a matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distance = pd.read_csv(r\"Data\\census\\station_distance_agg.csv\", index_col=[0])\n",
    "df_distance_mtx = pd.pivot_table(\n",
    "    df_distance, index='origin', columns='destination')\n",
    "\n",
    "df_distance_mtx.to_csv(r\"Data\\census\\station_distance_mtx_agg.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.gca()\n",
    "_ = zone_agg.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blue, red, and yellow route stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_station = [Point(42.116034, -86.452483),\n",
    "                Point(42.115074, -86.465348),\n",
    "                Point(42.110256, -86.463736),\n",
    "                Point(42.110254, -86.457768),\n",
    "                Point(42.101634, -86.448961),\n",
    "                Point(42.101646, -86.441657),\n",
    "                Point(42.102544, -86.436052),\n",
    "                Point(42.088709, -86.437108),\n",
    "                Point(42.084822, -86.437156),\n",
    "                Point(42.080667, -86.434759),\n",
    "                Point(42.085583, -86.433805),\n",
    "                Point(42.085637, -86.424232),\n",
    "                Point(42.082548, -86.421979),\n",
    "                Point(42.082242, -86.418849),\n",
    "                Point(42.077808, -86.424668),\n",
    "                Point(42.102544, -86.436052),\n",
    "                Point(42.107206, -86.446024),\n",
    "                Point(42.109733, -86.447242),\n",
    "                Point(42.116034, -86.452483),\n",
    "                ]\n",
    "blue_station = [Point(p.y, p.x) for p in blue_station]\n",
    "\n",
    "red_station = [Point(42.101646,\t-86.441657),\n",
    "               Point(42.101634,\t-86.448961),\n",
    "               Point(42.116034,\t-86.452483),\n",
    "               Point(42.115074,\t-86.465348),\n",
    "               Point(42.111264,\t-86.481872),\n",
    "               Point(42.088810,\t-86.478394),\n",
    "               Point(42.084126,\t-86.486379),\n",
    "               Point(42.079074,\t-86.493490),\n",
    "               Point(42.033439,\t-86.513542),\n",
    "               Point(42.026502,\t-86.516012),\n",
    "               Point(42.086425,\t-86.440537),\n",
    "               Point(42.101646,\t-86.441657),\n",
    "               ]\n",
    "red_station = [Point(p.y, p.x) for p in red_station]\n",
    "\n",
    "yellow_station = [Point(42.118494913335645, -86.45082973186932),\n",
    "                  Point(42.13082775201815, -86.4538851865351),\n",
    "                  Point(42.13268958444188, -86.45128880811971),\n",
    "                  Point(42.124573800847095, -86.4460383743168),\n",
    "                  Point(42.121903066372475, -86.4390957589761),\n",
    "                  Point(42.116026992072754, -86.4296080933503),\n",
    "                  Point(42.11587877166418, -86.43641202669362),\n",
    "                  Point(42.112791181420455, -86.4407060644722),\n",
    "                  Point(42.10241413329736, -86.43602474092258),\n",
    "                  Point(42.10241413, -86.43602474),\n",
    "                  Point(42.11279118, -86.44070606),\n",
    "                  Point(42.11587877, -86.43641203),\n",
    "                  Point(42.11602699, -86.42960809),\n",
    "                  Point(42.12190307, -86.43909576),\n",
    "                  Point(42.1245738,\t-86.44603837),\n",
    "                  Point(42.13268958, -86.45128881),\n",
    "                  Point(42.13082775\t, -86.45388519),\n",
    "                  Point(42.11849491, -86.45082973),\n",
    "                  ]\n",
    "yellow_station = [Point(p.y, p.x) for p in yellow_station]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_point(point, geoseries):\n",
    "    \"\"\"\n",
    "    Checker function which determines if a coordinate is within \n",
    "    the geoseries and return the zone id where the point is \n",
    "    located\n",
    "    \"\"\"\n",
    "    idx_list = geoseries.index[geoseries.geometry.contains(point)].tolist()\n",
    "    return idx_list[0] if len(idx_list) >= 1 else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(r\"Data\\shapefile\\zone_id_agg.shp\")\n",
    "blue_station_id = [gdf.loc[locate_point(p, gdf), \"zone_id\"] for p in blue_station]\n",
    "blue_station_id = [\n",
    "    v\n",
    "    for i, v in enumerate(blue_station_id)\n",
    "    if i < len(blue_station_id) - 1\n",
    "    if v != blue_station_id[i + 1]\n",
    "]\n",
    "\n",
    "blue_station_id += [gdf.loc[locate_point(blue_station[-1], gdf), \"zone_id\"]]\n",
    "\n",
    "red_station_id = [gdf.loc[locate_point(p, gdf), \"zone_id\"] for p in red_station]\n",
    "red_station_id = [\n",
    "    v\n",
    "    for i, v in enumerate(red_station_id)\n",
    "    if i < len(red_station_id) - 1\n",
    "    if v != red_station_id[i + 1]\n",
    "]\n",
    "\n",
    "red_station_id += [gdf.loc[locate_point(red_station[-1], gdf), \"zone_id\"]]\n",
    "\n",
    "yellow_station_id = [gdf.loc[locate_point(p, gdf), \"zone_id\"] for p in yellow_station]\n",
    "yellow_station_id = [\n",
    "    v\n",
    "    for i, v in enumerate(yellow_station_id)\n",
    "    if i < len(yellow_station_id) - 1\n",
    "    if v != yellow_station_id[i + 1]\n",
    "]\n",
    "\n",
    "yellow_station_id += [gdf.loc[locate_point(yellow_station[-1], gdf), \"zone_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load neighbor nodes information\n",
    "ctr_info = pickle.load(open(r\"Data\\temp\\Station.p\", \"rb\"))\n",
    "# graph with edge cost as shortest travel time\n",
    "G_t = pickle.load(open(r\"Data\\temp\\G_t.p\", \"rb\"))\n",
    "\n",
    "s_blue = list()\n",
    "s_blue.append(blue_station_id[0])\n",
    "for previous, current in zip(blue_station_id, blue_station_id[1:]):\n",
    "    if current in ctr_info[previous][\"neighbours\"]:\n",
    "        s_blue.append(current)\n",
    "    else:\n",
    "        sp = nx.shortest_path(G_t, source=previous, target=current, weight=\"weight\")\n",
    "        s_blue += sp[1:]\n",
    "\n",
    "s_red = list()\n",
    "s_red.append(red_station_id[0])\n",
    "for previous, current in zip(red_station_id, red_station_id[1:]):\n",
    "    if current in ctr_info[previous][\"neighbours\"]:\n",
    "        s_red.append(current)\n",
    "    else:\n",
    "        sp = nx.shortest_path(G_t, source=previous, target=current, weight=\"weight\")\n",
    "        s_red += sp[1:]\n",
    "\n",
    "s_yellow = list()\n",
    "s_yellow.append(yellow_station_id[0])\n",
    "for previous, current in zip(yellow_station_id, yellow_station_id[1:]):\n",
    "    if current in ctr_info[previous][\"neighbours\"]:\n",
    "        s_yellow.append(current)\n",
    "    else:\n",
    "        sp = nx.shortest_path(G_t, source=previous, target=current, weight=\"weight\")\n",
    "        s_yellow += sp[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get bus schedule time based on tau matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with edge cost as shortest travel time\n",
    "G_t = pickle.load(open(r\"Data\\temp\\G_t.p\", \"rb\"))\n",
    "\n",
    "S = 69  # number of zones\n",
    "DELTA_t = 1  # x min\n",
    "tau = np.zeros((S, S))\n",
    "tau2 = np.zeros((S, S))\n",
    "# round travel time to integer\n",
    "for _, _, d in G_t.edges(data=True):\n",
    "    d[\"weight\"] = np.rint(d[\"weight\"])\n",
    "\n",
    "for i in range(S):\n",
    "    for j in range(S):\n",
    "        if i == j:\n",
    "            tau[i, j] = 0\n",
    "            tau2[i, j] = 1\n",
    "        else:\n",
    "            tau[i, j] = (\n",
    "                nx.shortest_path_length(G_t, source=i, target=j, weight=\"weight\")\n",
    "                // DELTA_t\n",
    "            )\n",
    "            tau2[i, j] = tau[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch_blue_1 = np.cumsum(\n",
    "    [0]\n",
    "    + [\n",
    "        tau[s_blue[i], s_blue[i + 1]]\n",
    "        if s_blue[i] != s_blue[i + 1]\n",
    "        else 1\n",
    "        for i in range(len(s_blue) - 1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Rotate the blue bus station list with 30 mins\n",
    "idx = np.where(sch_blue_1 >= 30)[0][0]\n",
    "s_blue_2 = s_blue[-idx:] + s_blue[:-idx]\n",
    "\n",
    "sch_blue_2 = np.cumsum(\n",
    "    [0]\n",
    "    + [\n",
    "        tau[s_blue_2[i], s_blue_2[i + 1]]\n",
    "        if s_blue_2[i] != s_blue_2[i + 1]\n",
    "        else 1\n",
    "        for i in range(len(s_blue_2) - 1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "sch_red_1 = np.cumsum(\n",
    "    [0]\n",
    "    + [\n",
    "        tau[s_red[i], s_red[i + 1]]\n",
    "        if s_red[i] != s_red[i + 1]\n",
    "        else 1\n",
    "        for i in range(len(s_red) - 1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "sch_yellow_1 = np.cumsum(\n",
    "    [0]\n",
    "    + [\n",
    "        tau[s_yellow[i], s_yellow[i + 1]]\n",
    "        if s_yellow[i] != s_yellow[i + 1]\n",
    "        else 1\n",
    "        for i in range(len(s_yellow) - 1)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_fr_1 = {\"t\": sch_blue_1, \"s\": s_blue * 1}\n",
    "blue_fr_2 = {\"t\": sch_blue_2, \"s\": s_blue_2 * 1}\n",
    "\n",
    "df_blue_fr_1 = pd.DataFrame.from_dict(blue_fr_1)\n",
    "df_blue_fr_2 = pd.DataFrame.from_dict(blue_fr_2)\n",
    "\n",
    "red_fr_1 = {\"t\": sch_red_1, \"s\": s_red * 1}\n",
    "\n",
    "df_red_fr_1 = pd.DataFrame.from_dict(red_fr_1)\n",
    "\n",
    "yellow_fr_1 = {\"t\": sch_yellow_1, \"s\": s_yellow * 1}\n",
    "\n",
    "df_yellow_fr_1 = pd.DataFrame.from_dict(yellow_fr_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bus frequency for QGIS visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_freq = blue_station_id * 8\n",
    "red_freq = red_station_id * 5\n",
    "yellow_freq = yellow_station_id * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq = pd.DataFrame(index=np.arange(69), columns=[\"frequency\", ])\n",
    "df_freq[\"frequency\"] = df_freq[\"frequency\"].fillna(0)\n",
    "df_freq.index.name = \"zone_id\"\n",
    "for zone in blue_freq + red_freq + yellow_freq:\n",
    "    if pd.isna(df_freq.loc[zone, \"frequency\"]):\n",
    "        df_freq.loc[zone, \"frequency\"] = 0\n",
    "    else:\n",
    "        df_freq.loc[zone, \"frequency\"] += 1\n",
    "df_freq.to_csv(r\"Data\\\\bus_frequency.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate fixed route for blue, red and yellow bus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_blue_fr = [0, 4, 6, 9, 14, 15, 18, 23, 25, 27, 31, 33, 37, 40, 43, 52, 54, 56, 58]\n",
    "# t_blue_fr_1 = [tt for tt in t_blue_fr]\n",
    "# # t_blue_fr_1 += [tt + 60 for tt in t_blue_fr]\n",
    "# # t_blue_fr_1 += [tt + 120 for tt in t_blue_fr]\n",
    "\n",
    "# t_blue_fr_2 = [tt + 30 for tt in t_blue_fr]\n",
    "# # t_blue_fr_2 += [tt + 90 for tt in t_blue_fr]\n",
    "\n",
    "# blue_fr_1 = {\"t\": t_blue_fr_1, \"s\": blue_station_id * 1}\n",
    "# blue_fr_2 = {\"t\": t_blue_fr_2, \"s\": blue_station_id * 1}\n",
    "\n",
    "# df_blue_fr_1 = pd.DataFrame.from_dict(blue_fr_1)\n",
    "# df_blue_fr_2 = pd.DataFrame.from_dict(blue_fr_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_red_fr = [4, 6, 9, 15, 20, 30, 35, 40, 50, 52, 62, 63]\n",
    "# t_red_fr_1 = [tt for tt in t_red_fr]\n",
    "# # t_red_fr_1 += [tt + 60 for tt in t_red_fr]\n",
    "# # t_red_fr_1 += [tt + 120 for tt in t_red_fr]\n",
    "\n",
    "# red_fr_1 = {\"t\": t_red_fr_1, \"s\": red_station_id * 1}\n",
    "\n",
    "# df_red_fr_1 = pd.DataFrame.from_dict(red_fr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_yellow_fr = [0, 4, 6, 10, 12, 18, 21, 25, 29, 30, 34, 37, 39, 45, 48, 51, 53, 58]\n",
    "# t_yellow_fr_1 = [tt for tt in t_yellow_fr]\n",
    "# # t_yellow_fr_1 += [tt + 60 for tt in t_yellow_fr]\n",
    "# # t_yellow_fr_1 += [tt + 120 for tt in t_yellow_fr]\n",
    "\n",
    "# yellow_fr_1 = {\"t\": t_yellow_fr_1, \"s\": yellow_station_id * 1}\n",
    "\n",
    "# df_yellow_fr_1 = pd.DataFrame.from_dict(yellow_fr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blue_fr_1.to_csv(r\"many2many_data\\blue_fr_1.csv\", index=False)\n",
    "df_blue_fr_2.to_csv(r\"many2many_data\\blue_fr_2.csv\", index=False)\n",
    "df_red_fr_1.to_csv(r\"many2many_data\\red_fr_1.csv\", index=False)\n",
    "df_yellow_fr_1.to_csv(r\"many2many_data\\yellow_fr_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate last station to orgin station travel time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_blue_1 = tau[s_blue[-1], s_blue[0]]\n",
    "t_blue_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_blue_2 = tau[s_blue_2[-1], s_blue_2[0]]\n",
    "t_blue_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_red_1 = tau[s_red[-1], s_red[0]]\n",
    "t_red_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_yellow_1 = tau[s_yellow[-1], s_yellow[0]]\n",
    "t_yellow_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate mode choice data using Google Maps API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Google Maps API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your google map API key\n",
    "gmap_key = \"AIzaSyCAMHXBSQrYIppBxu_GAKCMhR4rYSPuB8k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### travel time and distance by walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = pickle.load(open(r\"Data\\temp\\Station.p\", \"rb\"))\n",
    "num_station = len(station)\n",
    "gmaps = googlemaps.Client(key=gmap_key)\n",
    "# datetime needs to be no later than current time, format: Y,M,D,h,m,s\n",
    "currenttime = datetime(2021, 6, 1, 12, 0, 0)\n",
    "\n",
    "# construct graphs of travel time and travel distance\n",
    "G_d = nx.Graph()\n",
    "G_t = nx.Graph()\n",
    "G_d.add_nodes_from(np.arange(num_station))\n",
    "G_t.add_nodes_from(np.arange(num_station))\n",
    "\n",
    "for i in range(num_station):\n",
    "    for j in station[i][\"neighbours\"]:\n",
    "        if i < j:\n",
    "            direction_result = gmaps.directions(\n",
    "                tuple(station[i][\"lat_lon\"]),\n",
    "                tuple(station[j][\"lat_lon\"]),\n",
    "                mode=\"walking\",\n",
    "                avoid=\"ferries\",\n",
    "                departure_time=currenttime,\n",
    "            )\n",
    "\n",
    "            w_t = np.round(\n",
    "                direction_result[0][\"legs\"][0][\"duration\"][u\"value\"] / 60.0, decimals=2\n",
    "            )\n",
    "            w_d = np.round(\n",
    "                direction_result[0][\"legs\"][0][\"distance\"][u\"value\"] * 0.000621371,\n",
    "                decimals=2,\n",
    "            )\n",
    "\n",
    "            G_t.add_edge(i, j, weight=w_t)\n",
    "            G_d.add_edge(i, j, weight=w_d)\n",
    "\n",
    "pickle.dump(G_t, open(r\"Data\\temp\\G_t_walk.p\", \"wb\"))\n",
    "pickle.dump(G_d, open(r\"Data\\temp\\G_d_walk.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ID converter\n",
    "id_converter = pickle.load(open(\"Data\\\\id_converter.p\", \"rb\"))\n",
    "id_converter_reverse = pickle.load(open(\"Data\\\\id_converter_reverse.p\", \"rb\"))\n",
    "df_O = pd.read_csv(\n",
    "    r\"mc_input_data\\dests_dat.csv\",\n",
    "    usecols=[\n",
    "        \"geoid_o\",\n",
    "    ],\n",
    ")\n",
    "df_D = pd.read_csv(r\"mc_input_data\\dests_geoid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with edge cost as shortest travel time (min)\n",
    "G_t_walk = pickle.load(open(r\"Data\\temp\\G_t_walk.p\", \"rb\"))\n",
    "\n",
    "time1 = [\n",
    "    nx.shortest_path_length(\n",
    "        G_t_walk,\n",
    "        source=id_converter_reverse[i],\n",
    "        target=id_converter_reverse[j],\n",
    "        weight=\"weight\",\n",
    "    )\n",
    "    if i in id_converter_reverse and j in id_converter_reverse\n",
    "    else 9999\n",
    "    for (i, j) in zip(df_O.geoid_o, df_D.geoid1)\n",
    "]\n",
    "\n",
    "time2 = [\n",
    "    nx.shortest_path_length(\n",
    "        G_t_walk,\n",
    "        source=id_converter_reverse[i],\n",
    "        target=id_converter_reverse[j],\n",
    "        weight=\"weight\",\n",
    "    )\n",
    "    if i in id_converter_reverse and j in id_converter_reverse\n",
    "    else 9999\n",
    "    for (i, j) in zip(df_O.geoid_o, df_D.geoid2)\n",
    "]\n",
    "\n",
    "time3 = [\n",
    "    nx.shortest_path_length(\n",
    "        G_t_walk,\n",
    "        source=id_converter_reverse[i],\n",
    "        target=id_converter_reverse[j],\n",
    "        weight=\"weight\",\n",
    "    )\n",
    "    if i in id_converter_reverse and j in id_converter_reverse\n",
    "    else 9999\n",
    "    for (i, j) in zip(df_O.geoid_o, df_D.geoid3)\n",
    "]\n",
    "\n",
    "time4 = [\n",
    "    nx.shortest_path_length(\n",
    "        G_t_walk,\n",
    "        source=id_converter_reverse[i],\n",
    "        target=id_converter_reverse[j],\n",
    "        weight=\"weight\",\n",
    "    )\n",
    "    if i in id_converter_reverse and j in id_converter_reverse\n",
    "    else 9999\n",
    "    for (i, j) in zip(df_O.geoid_o, df_D.geoid4)\n",
    "]\n",
    "\n",
    "time5 = [\n",
    "    nx.shortest_path_length(\n",
    "        G_t_walk,\n",
    "        source=id_converter_reverse[i],\n",
    "        target=id_converter_reverse[j],\n",
    "        weight=\"weight\",\n",
    "    )\n",
    "    if i in id_converter_reverse and j in id_converter_reverse\n",
    "    else 9999\n",
    "    for (i, j) in zip(df_O.geoid_o, df_D.geoid5)\n",
    "]\n",
    "\n",
    "df_walk_time_dat = pd.DataFrame(\n",
    "    data={\n",
    "        \"time1\": time1,\n",
    "        \"time2\": time2,\n",
    "        \"time3\": time3,\n",
    "        \"time4\": time4,\n",
    "        \"time5\": time5,\n",
    "    }\n",
    ")\n",
    "\n",
    "df_walk_time_dat.to_csv(r\"mc_input_data\\walk_time_dat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph with edge cost as shortest travel time (min)\n",
    "G_t = pickle.load(open(r\"Data\\temp\\G_t.p\", \"rb\"))\n",
    "\n",
    "time1 = [\n",
    "    nx.shortest_path_length(\n",
    "        G_t,\n",
    "        source=id_converter_reverse[i],\n",
    "        target=id_converter_reverse[j],\n",
    "        weight=\"weight\",\n",
    "    )\n",
    "    if i in id_converter_reverse and j in id_converter_reverse\n",
    "    else 9999\n",
    "    for (i, j) in zip(df_O.geoid_o, df_D.geoid1)\n",
    "]\n",
    "\n",
    "time2 = [\n",
    "    nx.shortest_path_length(\n",
    "        G_t,\n",
    "        source=id_converter_reverse[i],\n",
    "        target=id_converter_reverse[j],\n",
    "        weight=\"weight\",\n",
    "    )\n",
    "    if i in id_converter_reverse and j in id_converter_reverse\n",
    "    else 9999\n",
    "    for (i, j) in zip(df_O.geoid_o, df_D.geoid2)\n",
    "]\n",
    "\n",
    "time3 = [\n",
    "    nx.shortest_path_length(\n",
    "        G_t,\n",
    "        source=id_converter_reverse[i],\n",
    "        target=id_converter_reverse[j],\n",
    "        weight=\"weight\",\n",
    "    )\n",
    "    if i in id_converter_reverse and j in id_converter_reverse\n",
    "    else 9999\n",
    "    for (i, j) in zip(df_O.geoid_o, df_D.geoid3)\n",
    "]\n",
    "\n",
    "time4 = [\n",
    "    nx.shortest_path_length(\n",
    "        G_t,\n",
    "        source=id_converter_reverse[i],\n",
    "        target=id_converter_reverse[j],\n",
    "        weight=\"weight\",\n",
    "    )\n",
    "    if i in id_converter_reverse and j in id_converter_reverse\n",
    "    else 9999\n",
    "    for (i, j) in zip(df_O.geoid_o, df_D.geoid4)\n",
    "]\n",
    "\n",
    "time5 = [\n",
    "    nx.shortest_path_length(\n",
    "        G_t,\n",
    "        source=id_converter_reverse[i],\n",
    "        target=id_converter_reverse[j],\n",
    "        weight=\"weight\",\n",
    "    )\n",
    "    if i in id_converter_reverse and j in id_converter_reverse\n",
    "    else 9999\n",
    "    for (i, j) in zip(df_O.geoid_o, df_D.geoid5)\n",
    "]\n",
    "\n",
    "df_auto_time_dat = pd.DataFrame(\n",
    "    data={\n",
    "        \"time1\": time1,\n",
    "        \"time2\": time2,\n",
    "        \"time3\": time3,\n",
    "        \"time4\": time4,\n",
    "        \"time5\": time5,\n",
    "    }\n",
    ")\n",
    "\n",
    "foo = df_auto_time_dat.copy(deep=True)\n",
    "df_dar_time_dat = foo.applymap(lambda x: x * 40 / 35 if x != 9999 else 9999)\n",
    "\n",
    "df_dar_time_dat.to_csv(r\"mc_input_data\\dar_time_dat.csv\")\n",
    "df_auto_time_dat.to_csv(r\"mc_input_data\\auto_time_dat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "Summary drop off, pick up, and transfer frequency for each station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = r\"many2many_output\\results\\FR_and_1_BO_ATT_SPTT\\\\\"\n",
    "route0 = pd.read_csv(filedir + \"route_0_disagg.csv\").astype(np.int32)\n",
    "route1 = pd.read_csv(filedir + \"route_1_disagg.csv\").astype(np.int32)\n",
    "route2 = pd.read_csv(filedir + \"route_2_disagg.csv\").astype(np.int32)\n",
    "route3 = pd.read_csv(filedir + \"route_3_disagg.csv\").astype(np.int32)\n",
    "route4 = pd.read_csv(filedir + \"route_4_disagg.csv\").astype(np.int32)\n",
    "freq = dict()\n",
    "for s in pd.concat(\n",
    "    [route0.s1, route1.s1, route2.s1, route3.s1, route4.s1], ignore_index=True\n",
    ").values:\n",
    "    if s in freq:\n",
    "        freq[s] += 1\n",
    "    else:\n",
    "        freq[s] = 1\n",
    "\n",
    "agg_2_disagg_id = pickle.load(open(r\"Data\\\\agg_2_disagg_id.p\", \"rb\"))\n",
    "Y_df = pd.read_csv(\n",
    "    r\"many2many_output\\results\\FR_and_1_BO_ATT_SPTT\\Y_rdl_agg.csv\",\n",
    ")\n",
    "Y_df = Y_df.iloc[:-1, :].astype(np.int32)\n",
    "R = set(Y_df.r)\n",
    "station_on, station_off, station_transfer = dict(), dict(), dict()\n",
    "\n",
    "for r in R:\n",
    "    trajectory_r = Y_df.loc[Y_df.r == r, :]\n",
    "    if trajectory_r.s1.iloc[0] in station_on:\n",
    "        station_on[trajectory_r.s1.iloc[0]] += 1\n",
    "    else:\n",
    "        station_on[trajectory_r.s1.iloc[0]] = 1\n",
    "\n",
    "    if trajectory_r.s2.iloc[-1] in station_off:\n",
    "        station_off[trajectory_r.s2.iloc[-1]] += 1\n",
    "    else:\n",
    "        station_off[trajectory_r.s2.iloc[-1]] = 1\n",
    "\n",
    "    for index, row in trajectory_r.iterrows():\n",
    "        if (\n",
    "            index + 1 in trajectory_r.index\n",
    "            and row.d != trajectory_r.loc[index + 1, \"d\"]\n",
    "        ):\n",
    "            if row.s2 in station_transfer:\n",
    "                station_transfer[row.s2] += 1\n",
    "            else:\n",
    "                station_transfer[row.s2] = 1\n",
    "\n",
    "station_summary = pd.DataFrame(columns=(\"s\", \"on\", \"off\", \"transfer\"))\n",
    "S = set(station_on.keys())\n",
    "S.update(set(station_off.keys()))\n",
    "S.update(set(station_transfer.keys()))\n",
    "\n",
    "\n",
    "for s in S:\n",
    "    _ = station_on.setdefault(s, 0)\n",
    "    _ = station_off.setdefault(s, 0)\n",
    "    _ = station_transfer.setdefault(s, 0)\n",
    "    station_summary = station_summary.append(\n",
    "        {\n",
    "            \"s\": s,\n",
    "            \"on\": station_on[s],\n",
    "            \"off\": station_off[s],\n",
    "            \"transfer\": station_transfer[s],\n",
    "        },\n",
    "        ignore_index=True,\n",
    "    )\n",
    "station_summary.astype(np.float64).to_csv(\n",
    "    r\"E:\\Codes\\BH QGIS\\mc_m2m_att_sptt_agg\\station_summary_scen1.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = r\"many2many_output\\results\\FR_and_2_BO_ATT_SPTT\\\\\"\n",
    "Y_df = pd.read_csv(\n",
    "    filedir + \"Y_rdl_agg.csv\",\n",
    ")\n",
    "route0 = pd.read_csv(filedir + \"route_0_disagg.csv\").astype(np.int32)\n",
    "route1 = pd.read_csv(filedir + \"route_1_disagg.csv\").astype(np.int32)\n",
    "route2 = pd.read_csv(filedir + \"route_2_disagg.csv\").astype(np.int32)\n",
    "route3 = pd.read_csv(filedir + \"route_3_disagg.csv\").astype(np.int32)\n",
    "route4 = pd.read_csv(filedir + \"route_4_disagg.csv\").astype(np.int32)\n",
    "route5 = pd.read_csv(filedir + \"route_5_disagg.csv\").astype(np.int32)\n",
    "freq = dict()\n",
    "for s in pd.concat(\n",
    "    [\n",
    "        route0.s1,\n",
    "        route1.s1,\n",
    "        route2.s1,\n",
    "        route3.s1,\n",
    "        route4.s1,\n",
    "        route5.s1,\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ").values:\n",
    "    if s in freq:\n",
    "        freq[s] += 1\n",
    "    else:\n",
    "        freq[s] = 1\n",
    "\n",
    "        agg_2_disagg_id = pickle.load(open(r\"Data\\\\agg_2_disagg_id.p\", \"rb\"))\n",
    "\n",
    "Y_df = Y_df.iloc[:-1, :].astype(np.int32)\n",
    "R = set(Y_df.r)\n",
    "station_on, station_off, station_transfer = dict(), dict(), dict()\n",
    "\n",
    "for r in R:\n",
    "    trajectory_r = Y_df.loc[Y_df.r == r, :]\n",
    "    if trajectory_r.s1.iloc[0] in station_on:\n",
    "        station_on[trajectory_r.s1.iloc[0]] += 1\n",
    "    else:\n",
    "        station_on[trajectory_r.s1.iloc[0]] = 1\n",
    "\n",
    "    if trajectory_r.s2.iloc[-1] in station_off:\n",
    "        station_off[trajectory_r.s2.iloc[-1]] += 1\n",
    "    else:\n",
    "        station_off[trajectory_r.s2.iloc[-1]] = 1\n",
    "\n",
    "    for index, row in trajectory_r.iterrows():\n",
    "        if (\n",
    "            index + 1 in trajectory_r.index\n",
    "            and row.d != trajectory_r.loc[index + 1, \"d\"]\n",
    "        ):\n",
    "            if row.s2 in station_transfer:\n",
    "                station_transfer[row.s2] += 1\n",
    "            else:\n",
    "                station_transfer[row.s2] = 1\n",
    "station_summary = pd.DataFrame(columns=(\"s\", \"on\", \"off\", \"transfer\"))\n",
    "S = set(station_on.keys())\n",
    "S.update(set(station_off.keys()))\n",
    "S.update(set(station_transfer.keys()))\n",
    "\n",
    "\n",
    "for s in S:\n",
    "    _ = station_on.setdefault(s, 0)\n",
    "    _ = station_off.setdefault(s, 0)\n",
    "    _ = station_transfer.setdefault(s, 0)\n",
    "    station_summary = station_summary.append(\n",
    "        {\n",
    "            \"s\": s,\n",
    "            \"on\": station_on[s],\n",
    "            \"off\": station_off[s],\n",
    "            \"transfer\": station_transfer[s],\n",
    "        },\n",
    "        ignore_index=True,\n",
    "    )\n",
    "station_summary.astype(np.float64).to_csv(\n",
    "    r\"E:\\Codes\\BH QGIS\\mc_m2m_att_sptt_agg\\station_summary_scen2.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = r\"many2many_output\\results\\FR_and_3_BO_ATT_SPTT\\\\\"\n",
    "summary_file = r\"E:\\Codes\\BH QGIS\\mc_m2m_att_sptt_agg\\station_summary_scen3.csv\"\n",
    "Y_df = pd.read_csv(\n",
    "    filedir + \"Y_rdl_agg.csv\",\n",
    ")\n",
    "route0 = pd.read_csv(filedir + \"route_0_disagg.csv\").astype(np.int32)\n",
    "route1 = pd.read_csv(filedir + \"route_1_disagg.csv\").astype(np.int32)\n",
    "route2 = pd.read_csv(filedir + \"route_2_disagg.csv\").astype(np.int32)\n",
    "route3 = pd.read_csv(filedir + \"route_3_disagg.csv\").astype(np.int32)\n",
    "route4 = pd.read_csv(filedir + \"route_4_disagg.csv\").astype(np.int32)\n",
    "route5 = pd.read_csv(filedir + \"route_5_disagg.csv\").astype(np.int32)\n",
    "route6 = pd.read_csv(filedir + \"route_6_disagg.csv\").astype(np.int32)\n",
    "freq = dict()\n",
    "for s in pd.concat(\n",
    "    [\n",
    "        route0.s1,\n",
    "        route1.s1,\n",
    "        route2.s1,\n",
    "        route3.s1,\n",
    "        route4.s1,\n",
    "        route5.s1,\n",
    "        route6.s1,\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ").values:\n",
    "    if s in freq:\n",
    "        freq[s] += 1\n",
    "    else:\n",
    "        freq[s] = 1\n",
    "\n",
    "        agg_2_disagg_id = pickle.load(open(r\"Data\\\\agg_2_disagg_id.p\", \"rb\"))\n",
    "\n",
    "Y_df = Y_df.iloc[:-1, :].astype(np.int32)\n",
    "R = set(Y_df.r)\n",
    "station_on, station_off, station_transfer = dict(), dict(), dict()\n",
    "\n",
    "for r in R:\n",
    "    trajectory_r = Y_df.loc[Y_df.r == r, :]\n",
    "    if trajectory_r.s1.iloc[0] in station_on:\n",
    "        station_on[trajectory_r.s1.iloc[0]] += 1\n",
    "    else:\n",
    "        station_on[trajectory_r.s1.iloc[0]] = 1\n",
    "\n",
    "    if trajectory_r.s2.iloc[-1] in station_off:\n",
    "        station_off[trajectory_r.s2.iloc[-1]] += 1\n",
    "    else:\n",
    "        station_off[trajectory_r.s2.iloc[-1]] = 1\n",
    "\n",
    "    for index, row in trajectory_r.iterrows():\n",
    "        if (\n",
    "            index + 1 in trajectory_r.index\n",
    "            and row.d != trajectory_r.loc[index + 1, \"d\"]\n",
    "        ):\n",
    "            if row.s2 in station_transfer:\n",
    "                station_transfer[row.s2] += 1\n",
    "            else:\n",
    "                station_transfer[row.s2] = 1\n",
    "station_summary = pd.DataFrame(columns=(\"s\", \"on\", \"off\", \"transfer\"))\n",
    "S = set(station_on.keys())\n",
    "S.update(set(station_off.keys()))\n",
    "S.update(set(station_transfer.keys()))\n",
    "\n",
    "\n",
    "for s in S:\n",
    "    _ = station_on.setdefault(s, 0)\n",
    "    _ = station_off.setdefault(s, 0)\n",
    "    _ = station_transfer.setdefault(s, 0)\n",
    "    station_summary = station_summary.append(\n",
    "        {\n",
    "            \"s\": s,\n",
    "            \"on\": station_on[s],\n",
    "            \"off\": station_off[s],\n",
    "            \"transfer\": station_transfer[s],\n",
    "        },\n",
    "        ignore_index=True,\n",
    "    )\n",
    "station_summary.astype(np.float64).to_csv(\n",
    "    summary_file,\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = r\"many2many_output\\results\\FR_and_4_BO_ATT_SPTT\\\\\"\n",
    "summary_file = r\"E:\\Codes\\BH QGIS\\mc_m2m_att_sptt_agg\\station_summary_scen4.csv\"\n",
    "Y_df = pd.read_csv(\n",
    "    filedir + \"Y_rdl_agg.csv\",\n",
    ")\n",
    "route0 = pd.read_csv(filedir + \"route_0_disagg.csv\").astype(np.int32)\n",
    "route1 = pd.read_csv(filedir + \"route_1_disagg.csv\").astype(np.int32)\n",
    "route2 = pd.read_csv(filedir + \"route_2_disagg.csv\").astype(np.int32)\n",
    "route3 = pd.read_csv(filedir + \"route_3_disagg.csv\").astype(np.int32)\n",
    "route4 = pd.read_csv(filedir + \"route_4_disagg.csv\").astype(np.int32)\n",
    "route5 = pd.read_csv(filedir + \"route_5_disagg.csv\").astype(np.int32)\n",
    "route6 = pd.read_csv(filedir + \"route_6_disagg.csv\").astype(np.int32)\n",
    "route7 = pd.read_csv(filedir + \"route_7_disagg.csv\").astype(np.int32)\n",
    "freq = dict()\n",
    "for s in pd.concat(\n",
    "    [\n",
    "        route0.s1,\n",
    "        route1.s1,\n",
    "        route2.s1,\n",
    "        route3.s1,\n",
    "        route4.s1,\n",
    "        route5.s1,\n",
    "        route6.s1,\n",
    "        route7.s1,\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ").values:\n",
    "    if s in freq:\n",
    "        freq[s] += 1\n",
    "    else:\n",
    "        freq[s] = 1\n",
    "\n",
    "        agg_2_disagg_id = pickle.load(open(r\"Data\\\\agg_2_disagg_id.p\", \"rb\"))\n",
    "\n",
    "Y_df = Y_df.iloc[:-1, :].astype(np.int32)\n",
    "R = set(Y_df.r)\n",
    "station_on, station_off, station_transfer = dict(), dict(), dict()\n",
    "\n",
    "for r in R:\n",
    "    trajectory_r = Y_df.loc[Y_df.r == r, :]\n",
    "    if trajectory_r.s1.iloc[0] in station_on:\n",
    "        station_on[trajectory_r.s1.iloc[0]] += 1\n",
    "    else:\n",
    "        station_on[trajectory_r.s1.iloc[0]] = 1\n",
    "\n",
    "    if trajectory_r.s2.iloc[-1] in station_off:\n",
    "        station_off[trajectory_r.s2.iloc[-1]] += 1\n",
    "    else:\n",
    "        station_off[trajectory_r.s2.iloc[-1]] = 1\n",
    "\n",
    "    for index, row in trajectory_r.iterrows():\n",
    "        if (\n",
    "            index + 1 in trajectory_r.index\n",
    "            and row.d != trajectory_r.loc[index + 1, \"d\"]\n",
    "        ):\n",
    "            if row.s2 in station_transfer:\n",
    "                station_transfer[row.s2] += 1\n",
    "            else:\n",
    "                station_transfer[row.s2] = 1\n",
    "station_summary = pd.DataFrame(columns=(\"s\", \"on\", \"off\", \"transfer\"))\n",
    "S = set(station_on.keys())\n",
    "S.update(set(station_off.keys()))\n",
    "S.update(set(station_transfer.keys()))\n",
    "\n",
    "\n",
    "for s in S:\n",
    "    _ = station_on.setdefault(s, 0)\n",
    "    _ = station_off.setdefault(s, 0)\n",
    "    _ = station_transfer.setdefault(s, 0)\n",
    "    station_summary = station_summary.append(\n",
    "        {\n",
    "            \"s\": s,\n",
    "            \"on\": station_on[s],\n",
    "            \"off\": station_off[s],\n",
    "            \"transfer\": station_transfer[s],\n",
    "        },\n",
    "        ignore_index=True,\n",
    "    )\n",
    "station_summary.astype(np.float64).to_csv(\n",
    "    summary_file,\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = r\"many2many_output\\results\\FR_and_5_BO_ATT_SPTT\\\\\"\n",
    "summary_file = r\"E:\\Codes\\BH QGIS\\mc_m2m_att_sptt_agg\\station_summary_scen5.csv\"\n",
    "Y_df = pd.read_csv(\n",
    "    filedir + \"Y_rdl_agg.csv\",\n",
    ")\n",
    "route0 = pd.read_csv(filedir + \"route_0_disagg.csv\").astype(np.int32)\n",
    "route1 = pd.read_csv(filedir + \"route_1_disagg.csv\").astype(np.int32)\n",
    "route2 = pd.read_csv(filedir + \"route_2_disagg.csv\").astype(np.int32)\n",
    "route3 = pd.read_csv(filedir + \"route_3_disagg.csv\").astype(np.int32)\n",
    "route4 = pd.read_csv(filedir + \"route_4_disagg.csv\").astype(np.int32)\n",
    "route5 = pd.read_csv(filedir + \"route_5_disagg.csv\").astype(np.int32)\n",
    "route6 = pd.read_csv(filedir + \"route_6_disagg.csv\").astype(np.int32)\n",
    "route7 = pd.read_csv(filedir + \"route_7_disagg.csv\").astype(np.int32)\n",
    "route8 = pd.read_csv(filedir + \"route_8_disagg.csv\").astype(np.int32)\n",
    "freq = dict()\n",
    "for s in pd.concat(\n",
    "    [\n",
    "        route0.s1,\n",
    "        route1.s1,\n",
    "        route2.s1,\n",
    "        route3.s1,\n",
    "        route4.s1,\n",
    "        route5.s1,\n",
    "        route6.s1,\n",
    "        route7.s1,\n",
    "        route8.s1,\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ").values:\n",
    "    if s in freq:\n",
    "        freq[s] += 1\n",
    "    else:\n",
    "        freq[s] = 1\n",
    "\n",
    "        agg_2_disagg_id = pickle.load(open(r\"Data\\\\agg_2_disagg_id.p\", \"rb\"))\n",
    "\n",
    "Y_df = Y_df.iloc[:-1, :].astype(np.int32)\n",
    "R = set(Y_df.r)\n",
    "station_on, station_off, station_transfer = dict(), dict(), dict()\n",
    "\n",
    "for r in R:\n",
    "    trajectory_r = Y_df.loc[Y_df.r == r, :]\n",
    "    if trajectory_r.s1.iloc[0] in station_on:\n",
    "        station_on[trajectory_r.s1.iloc[0]] += 1\n",
    "    else:\n",
    "        station_on[trajectory_r.s1.iloc[0]] = 1\n",
    "\n",
    "    if trajectory_r.s2.iloc[-1] in station_off:\n",
    "        station_off[trajectory_r.s2.iloc[-1]] += 1\n",
    "    else:\n",
    "        station_off[trajectory_r.s2.iloc[-1]] = 1\n",
    "\n",
    "    for index, row in trajectory_r.iterrows():\n",
    "        if (\n",
    "            index + 1 in trajectory_r.index\n",
    "            and row.d != trajectory_r.loc[index + 1, \"d\"]\n",
    "        ):\n",
    "            if row.s2 in station_transfer:\n",
    "                station_transfer[row.s2] += 1\n",
    "            else:\n",
    "                station_transfer[row.s2] = 1\n",
    "station_summary = pd.DataFrame(columns=(\"s\", \"on\", \"off\", \"transfer\"))\n",
    "S = set(station_on.keys())\n",
    "S.update(set(station_off.keys()))\n",
    "S.update(set(station_transfer.keys()))\n",
    "\n",
    "\n",
    "for s in S:\n",
    "    _ = station_on.setdefault(s, 0)\n",
    "    _ = station_off.setdefault(s, 0)\n",
    "    _ = station_transfer.setdefault(s, 0)\n",
    "    station_summary = station_summary.append(\n",
    "        {\n",
    "            \"s\": s,\n",
    "            \"on\": station_on[s],\n",
    "            \"off\": station_off[s],\n",
    "            \"transfer\": station_transfer[s],\n",
    "        },\n",
    "        ignore_index=True,\n",
    "    )\n",
    "station_summary.astype(np.float64).to_csv(\n",
    "    summary_file,\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = r\"many2many_output\\results\\FR_and_6_BO_ATT_SPTT\\\\\"\n",
    "summary_file = r\"E:\\Codes\\BH QGIS\\mc_m2m_att_sptt_agg\\station_summary_scen6.csv\"\n",
    "Y_df = pd.read_csv(\n",
    "    filedir + \"Y_rdl_agg.csv\",\n",
    ")\n",
    "route0 = pd.read_csv(filedir + \"route_0_disagg.csv\").astype(np.int32)\n",
    "route1 = pd.read_csv(filedir + \"route_1_disagg.csv\").astype(np.int32)\n",
    "route2 = pd.read_csv(filedir + \"route_2_disagg.csv\").astype(np.int32)\n",
    "route3 = pd.read_csv(filedir + \"route_3_disagg.csv\").astype(np.int32)\n",
    "route4 = pd.read_csv(filedir + \"route_4_disagg.csv\").astype(np.int32)\n",
    "route5 = pd.read_csv(filedir + \"route_5_disagg.csv\").astype(np.int32)\n",
    "route6 = pd.read_csv(filedir + \"route_6_disagg.csv\").astype(np.int32)\n",
    "route7 = pd.read_csv(filedir + \"route_7_disagg.csv\").astype(np.int32)\n",
    "route8 = pd.read_csv(filedir + \"route_8_disagg.csv\").astype(np.int32)\n",
    "route9 = pd.read_csv(filedir + \"route_9_disagg.csv\").astype(np.int32)\n",
    "freq = dict()\n",
    "for s in pd.concat(\n",
    "    [\n",
    "        route0.s1,\n",
    "        route1.s1,\n",
    "        route2.s1,\n",
    "        route3.s1,\n",
    "        route4.s1,\n",
    "        route5.s1,\n",
    "        route6.s1,\n",
    "        route7.s1,\n",
    "        route8.s1,\n",
    "        route9.s1,\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ").values:\n",
    "    if s in freq:\n",
    "        freq[s] += 1\n",
    "    else:\n",
    "        freq[s] = 1\n",
    "\n",
    "        agg_2_disagg_id = pickle.load(open(r\"Data\\\\agg_2_disagg_id.p\", \"rb\"))\n",
    "\n",
    "Y_df = Y_df.iloc[:-1, :].astype(np.int32)\n",
    "R = set(Y_df.r)\n",
    "station_on, station_off, station_transfer = dict(), dict(), dict()\n",
    "\n",
    "for r in R:\n",
    "    trajectory_r = Y_df.loc[Y_df.r == r, :]\n",
    "    if trajectory_r.s1.iloc[0] in station_on:\n",
    "        station_on[trajectory_r.s1.iloc[0]] += 1\n",
    "    else:\n",
    "        station_on[trajectory_r.s1.iloc[0]] = 1\n",
    "\n",
    "    if trajectory_r.s2.iloc[-1] in station_off:\n",
    "        station_off[trajectory_r.s2.iloc[-1]] += 1\n",
    "    else:\n",
    "        station_off[trajectory_r.s2.iloc[-1]] = 1\n",
    "\n",
    "    for index, row in trajectory_r.iterrows():\n",
    "        if (\n",
    "            index + 1 in trajectory_r.index\n",
    "            and row.d != trajectory_r.loc[index + 1, \"d\"]\n",
    "        ):\n",
    "            if row.s2 in station_transfer:\n",
    "                station_transfer[row.s2] += 1\n",
    "            else:\n",
    "                station_transfer[row.s2] = 1\n",
    "station_summary = pd.DataFrame(columns=(\"s\", \"on\", \"off\", \"transfer\"))\n",
    "S = set(station_on.keys())\n",
    "S.update(set(station_off.keys()))\n",
    "S.update(set(station_transfer.keys()))\n",
    "\n",
    "\n",
    "for s in S:\n",
    "    _ = station_on.setdefault(s, 0)\n",
    "    _ = station_off.setdefault(s, 0)\n",
    "    _ = station_transfer.setdefault(s, 0)\n",
    "    station_summary = station_summary.append(\n",
    "        {\n",
    "            \"s\": s,\n",
    "            \"on\": station_on[s],\n",
    "            \"off\": station_off[s],\n",
    "            \"transfer\": station_transfer[s],\n",
    "        },\n",
    "        ignore_index=True,\n",
    "    )\n",
    "station_summary.astype(np.float64).to_csv(\n",
    "    summary_file,\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary trip table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 69  # number of zones\n",
    "trip_table_dir = r\"E:\\Codes\\BH_data_preprocess_and_DP\\many2many_data\\FR_and_6_BO_ATT_SPTT\\\\\"\n",
    "transit_trips_dict_pk = pickle.load(\n",
    "    open(trip_table_dir + \"transit_trips_dict_pk.p\", \"rb\"),\n",
    ")\n",
    "\n",
    "trip_sum = pd.DataFrame(columns=[\"zone_id\", \"indegree\", \"outdegree\"])\n",
    "trip_sum.zone_id = np.arange(S)\n",
    "for station in np.arange(S):\n",
    "    trip_sum.loc[trip_sum.zone_id == station, \"indegree\"] = sum(\n",
    "        [trips for (o, d), trips in transit_trips_dict_pk.items() if d == station]\n",
    "    )\n",
    "    trip_sum.loc[trip_sum.zone_id == station, \"outdegree\"] = sum(\n",
    "        [trips for (o, d), trips in transit_trips_dict_pk.items() if o == station]\n",
    "    )\n",
    "trip_sum.to_csv(trip_table_dir + \"transit_trips_dict_pk.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot colormap of indegree and outdegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_dir = r\"E:\\Codes\\BH_data_preprocess_and_DP\\Data\\shapefile\\\\\"\n",
    "gpd_shp_file = gpd.read_file(shapefile_dir + r\"zone_id.shp\")\n",
    "gpd_shp_trip_sum = gpd_shp_file.merge(right=trip_sum, how=\"left\", on=\"zone_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 25))\n",
    "ax = fig.gca()\n",
    "\n",
    "vmin = gpd_shp_trip_sum.indegree.min()\n",
    "vmax = gpd_shp_trip_sum.indegree.max()\n",
    "\n",
    "gpd_shp_trip_sum.plot(\n",
    "    column=\"indegree\",\n",
    "    ax=ax,\n",
    "    legend=False,\n",
    "    cmap=\"Reds\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "\n",
    "cax = fig.add_axes([1, 0.1, 0.03, 0.8])\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=\"Reds\", norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "sm._A = []\n",
    "cbr = fig.colorbar(\n",
    "    sm,\n",
    "    cax=cax,\n",
    ")\n",
    "cbr.ax.tick_params(labelsize=30)\n",
    "\n",
    "fig.savefig(\n",
    "    trip_table_dir + r\"transit_trip_destinations.png\", bbox_inches=\"tight\", dpi=100\n",
    ")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 25))\n",
    "ax = fig.gca()\n",
    "\n",
    "vmin = gpd_shp_trip_sum.outdegree.min()\n",
    "vmax = gpd_shp_trip_sum.outdegree.max()\n",
    "\n",
    "gpd_shp_trip_sum.plot(\n",
    "    column=\"outdegree\",\n",
    "    ax=ax,\n",
    "    legend=False,\n",
    "    cmap=\"Blues\",\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "\n",
    "cax = fig.add_axes([1, 0.1, 0.03, 0.8])\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=\"Blues\", norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "sm._A = []\n",
    "cbr = fig.colorbar(\n",
    "    sm,\n",
    "    cax=cax,\n",
    ")\n",
    "cbr.ax.tick_params(labelsize=30)\n",
    "\n",
    "fig.savefig(\n",
    "    trip_table_dir + r\"transit_trip_origins.png\", bbox_inches=\"tight\", dpi=100\n",
    ")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
